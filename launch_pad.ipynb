{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc1f194-0163-4234-9eea-c75e8973e728",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15c117a-2810-456a-b956-86d2985385d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import sys\n",
    "import json\n",
    "import entry_point\n",
    "import importlib\n",
    "import dbks_dbms.mgr.dbks_dms_mgr\n",
    "import dbks_dbms.stg_tbl_mgr.mysql_tbl_mgr\n",
    "import dbks_dbms.stg_tbl_mgr.redshift_tbl_mgr\n",
    "import dbks_dbms.mysql_rw.mysql_writer\n",
    "from dbks_dbms.stg_tbl_mgr.mysql_tbl_mgr import MysqlTblMgr\n",
    "from dbks_dbms.mysql_rw.mysql_writer import MysqlWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "970c0f09-b321-4c9f-b676-e3d6765c9e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "src_table_name=\"dw_dim_state\"\n",
    "src_database=\"conformedpiiedw\"\n",
    "tgt_db_typ=\"redshift\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37d37ea-f51e-4b8b-87da-8f072d7c59a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('database_path_mapping.json', 'r') as f:\n",
    "    database_path_mapping = json.load(f)\n",
    "\n",
    "src_database_path=database_path_mapping.get(src_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01610e47-318a-44d6-acd2-ddc13617d9e7",
     "showTitle": false,
     "title": ""
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_current_notebook_path():\n",
    "    notebook_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    return notebook_path\n",
    "\n",
    "def get_environment_from_notebook_path():\n",
    "    #note_book_path = get_current_notebook_path()\n",
    "    #main_logger.info(f\"Notebook path :{note_book_path}\")\n",
    "    if  spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")=='4022166418081681':\n",
    "        return 'PT'\n",
    "    elif spark.conf.get(\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\")=='add production id':\n",
    "        return 'prod'\n",
    "\n",
    "def get_user_mysql_cred(mysql_env,secret_mgr_scope='ds-edw-prod'):\n",
    "    if mysql_env.lower() in ['pt','prod']:\n",
    "        mysql_ip=dbutils.secrets.get(scope='ds-edw-prod', key='HOST_NAME')#'edwawsdatasci01.aws2.teladoc.com' #dbutils.secrets.get(scope=secret_mgr_scope, key='HOST_NAME')\n",
    "        mysql_port=dbutils.secrets.get(scope='ds-edw-prod', key='HOST_PORT')#3306 #dbutils.secrets.get(scope=secret_mgr_scope, key='HOST_PORT')\n",
    "        musql_username=dbutils.secrets.get(scope='ds-edw-prod', key='USERNAME')#'databricks' #dbutils.secrets.get(scope=secret_mgr_scope, key='USERNAME')\n",
    "        mysql_password=dbutils.secrets.get(scope='ds-edw-prod', key='PASSWORD') #dbutils.secrets.get(scope=secret_mgr_scope, key='PASSWORD')\n",
    "        return (mysql_ip,mysql_port,musql_username,mysql_password)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid environment for mysql user credentials {mysql_env}\")\n",
    "\n",
    "def get_redshift_user_cred(redshift_env,secret_mgr_scope):\n",
    "    if redshift_env.lower() in ['prod']:\n",
    "        host='10.10.75.66'\n",
    "        #dbutils.secrets.get(scope=secret_mgr_scope, key='HOST_IP')\n",
    "        port= 5439\n",
    "        #dbutils.secrets.get(scope=secret_mgr_scope, key='HOST_PORT')\n",
    "        username='databricks_pii'\n",
    "        #dbutils.secrets.get(scope=secret_mgr_scope, key='USERNAME')\n",
    "        password='XcMirz9XrKKt-TTDqhnTGVgTU'\n",
    "        #dbutils.secrets.get(scope=secret_mgr_scope, key='PASSWORD')\n",
    "        return (host,port,username,password)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid environment for redshift user credentials {redshift_env}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128e66d4-2c40-4a1e-b2d1-6ec4188402a9",
     "showTitle": false,
     "title": ""
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importlib.reload(entry_point)\n",
    "environment = get_environment_from_notebook_path()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e7793a-f944-4c64-a292-8ce917ff3581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "pathlist = ['/Workspace/Users/madan.hajare@teladochealth.com/dbks_dbms','/Workspace/Users/madan.hajare@teladochealth.com/dbks_dbms/mgr','/Workspace/Users/madan.hajare@teladochealth.com/dbks_dbms/mgr/dbks_dms_mgr','/Workspace/Users/madan.hajare@teladochealth.com/dbks_dbms/stg_tbl_mgr/mysql_tbl_mgr'\n",
    "            ,'dbks_dbms/mysql_rw/mysql_writer']\n",
    "\n",
    "for path in pathlist:\n",
    "    if path not in sys.path:\n",
    "        print(\"Adding path: \" + path)\n",
    "        sys.path.append(path)\n",
    "\n",
    "\n",
    "importlib.reload(dbks_dbms.mgr.dbks_dms_mgr)\n",
    "importlib.reload(dbks_dbms.stg_tbl_mgr.mysql_tbl_mgr)\n",
    "importlib.reload(dbks_dbms.mysql_rw.mysql_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72da289d-9d0c-40de-8512-4a37a75ff173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    assert src_database_path, f\"source database path for {src_database} is not set in database_path_mapping.json\"\n",
    "\n",
    "    df=spark.table(\"udf_internal.udf_to_rdbms_sync\").where(f\"src_db='{src_database_path}'\").where(f\"src_tbl='{src_table_name}'\").where(f\"tgt_db_typ='{tgt_db_typ}'\").select(\"udf_to_rdbms_sync_key\",\"src_db\",\"src_tbl\",\"tgt_db_typ\",\"tgt_db\",\"tgt_tbl\",\"merge_keys\")\n",
    "\n",
    "\n",
    "    assert df.count() > 0, f\"No records found in udf_internal.udf_to_rdbms_sync for src_db={src_database} and src_tbl={src_table_name} and tgt_db_typ={tgt_db_typ}\"\n",
    "\n",
    "    # check last successful log entry and get watermark value\n",
    "\n",
    "    mysql_ip,mysql_port,mysql_username,mysql_password = get_user_mysql_cred(mysql_env=environment,secret_mgr_scope='ds-edw-prod')\n",
    "    redshift_ip,redshift_port,redshift_username,redshift_password = get_redshift_user_cred(redshift_env='prod',secret_mgr_scope='ds-rs-pii-prod')\n",
    "\n",
    "    for row in df.collect():\n",
    "            conn= (mysql_ip,mysql_port,mysql_username,mysql_password,redshift_ip,redshift_port,redshift_username,redshift_password)    \n",
    "            master_obj= entry_point.start_dbks_dms(row,conn)\n",
    "except Exception as e:\n",
    "    data=(None,None,'fail',None,datetime.now(),datetime.now(),'UDF_SYNC_FRMWRK','UDF_SYNC_FRMWRK',e.args[0])\n",
    "    schema = spark.table(\"udf_internal.udf_to_rdbms_sync_log\").schema\n",
    "    spark.createDataFrame([data],schema).write.mode(\"append\").saveAsTable(\"udf_internal.udf_to_rdbms_sync_log\")\n",
    "  \n",
    "                                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e935a82-d3b1-44fc-a0d5-e40e92f1dc58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * \n",
    "from  udf_internal.udf_to_rdbms_sync\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29271e62-9077-4fb2-9fbe-0863948dbee4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * \n",
    "from udf_internal.udf_to_rdbms_sync_log\n",
    "order by udf_created_dt desc \n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0095a196-df46-4966-8de8-8db8808c93d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Redshift connection details\n",
    "redshift_jdbc_url = f\"jdbc:redshift://{dbutils.secrets.get(scope='ds-rs-pii-prod', key='HOST_IP')}:5439/prod\"\n",
    "redshift_properties = {\n",
    "    \"user\": dbutils.secrets.get(scope='ds-rs-pii-prod', key='USERNAME'),\n",
    "    \"password\": dbutils.secrets.get(scope='ds-rs-pii-prod', key='PASSWORD'),\n",
    "    \"driver\": \"com.databricks.spark.redshift\"\n",
    "}\n",
    "\n",
    "# SQL query to get the current date and timestamp in Redshift\n",
    "test_query = \"(SELECT CURRENT_DATE, CURRENT_TIMESTAMP) as test_query\"\n",
    "\n",
    "# Execute the query using JDBC\n",
    "result_df = spark.read \\\n",
    "    .jdbc(redshift_jdbc_url, test_query, properties=redshift_properties)\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1946800546500056,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "launch_pad",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
